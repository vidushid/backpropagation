# Implementing Back Propagation Fram Scratch

The [dataset](leap.ee.iisc.ac.in/sriram/teaching/MLSP22/assignments/HW4/Data.tar.gz) provided (training and test subject faces with happy/sad emotions, each image of size 100 Ã— 100) was used. I applied PCA to reduce the input dimensionality from 10,000 to K = 12. A one-hidden-layer neural network was implemented with 10 hidden neurons, using ReLU activations in the hidden layer and softmax activations at the output. The network was trained for 20 iterations using backpropagation (coded from scratch, without external libraries), optimizing the cross-entropy loss. The training error was plotted as a function of iteration. Finally, the test accuracy was evaluated, and the change in performance was analyzed when the number of hidden neurons was increased from 10 to 15.

